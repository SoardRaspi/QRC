**Quantum-Hybrid Mixture-Based Network for Quantum Machine Learning**

In my journey of learning about Quantum Computing and Quantum Machine Learning, I have seen quite a few projects and research papers, using the concept of Quantum-Hybrid networks or circuits with classical structures to enhance the different Machine Learning ideas. These ideas mostly use separate/isolated Quantum and Classical circuits and then combine them in a sequential or parallel configuration. As an example, if we talk about variational quantum circuits for machine learning, I have mostly seen the use of transfer learning, either majorly quantum variational circuits optimized by classical algorithms or majorly classical circuit(s) taking input as the output of some other quantum system .eg. in the case of Quantum Convolutional Networks or Kernels. Recently, there has also been some progress in the field of implementing the concept of spiking-neurons in the Quantum paradigm. Taking inspiration from these recent developments, a slightly unique idea that I had thought of was the implementation of a **real** Quantum-Hybrid network in Neural Networks.

Talking about the field of Quantum Machine Learning (QML), the idea can be seen as a mixture of perceptrons and a few qubits in each of the layers. Between two consecutive layers, the idea proposes entanglement of qubits from one layer to those of the other layer. Graphically, the implementations looks like:

![Quantum RC](https://github.com/SoardRaspi/Quantum-RC-Reservoir-Computing-/blob/main/Bloq%20Crazy.png)

In the above diagram, in each layer, there are two types of 'circles' with a different label. In the black-filled circle, the naming followed is pl_i and in the white-filled circle, the naming followed is ql_i. Here, 'p' represents a simple perceptron which is used in the normal neural networks and 'q' represents the quibt which is being used along-side the perceptrons in the layer. In the subscript, 'l' represents the layer number and 'i' represents the number of perceptron or qubit in that layer.

Between two layers, in the above case layer 1 and 2, the connections between the neurons are not shown as they are well-understood. However, the dotted lines shown, represent the entanglement(s) between the qubits of the different layers. The potential benefit they provide is the all-together different type of embedding and the processing of the qubits. The entanglement is believed to help in changing the weight(s)/embedding(s) in the layers. This would be like back-propagation, only faster. I really believe that this design could help in bringing some new insights in Quantum Machine Learning and could help in faster training of the networks.

To make it more natural or similar to the way the human brain functions, a few connections among the components of a single layer could be made as well. This could mean an increase in the computational needs while training and inference of the model. However, introducing random connections among the components using spiking neurons would be an interesting introduction in the existing architecture. Elaborating on this, the spiking neurons should be connected between the outputs of some neurons to the input of the other neurons in the same layer. This would mean that either within a layer, some components (source of the spiking neurons) should be computed first and then the others (those taking the output of these spiking neurons as their input) later. Or, any layer which contains such spiking connections needs to be computed twice, first with active spiking connections and next with normal computations (keeping the spiking connections inactive).

This proposed method can also be used as a reservoir instead of the layers in the Neural Network. Having a quantum-classic hybrid reservoir with semi-random spiking connections could bring new insights in Reservoir Computing.
